{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Proteomics analysis (MACSima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 MACSima reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import harpy as hp\n",
    "from spatialdata_io import macsima\n",
    "\n",
    "input_path =  r\"D:\\example_data_MACSima\\REAscreen_IO_CRC\\REAscreen_IO_CRC\" # Set input path to folder containing MACSima images\n",
    "output_path = r\"D:\\example_data_MACSima\\REAscreen_IO_CRC\\output_harpy\" # Set a path to save the output from Harpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in MACSima data using the spatialdata-io macsima reader\n",
    "sdata = macsima(\n",
    "    path = input_path,\n",
    "    split_threshold_nuclei_channel=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to make sure the SpatialData opject is backed to disk (which is not included in the reader function)\n",
    "import os\n",
    "from spatialdata import read_zarr\n",
    "\n",
    "# Write the SpatialData object to disk\n",
    "sdata.write(os.path.join(output_path, \"sdata.zarr\"))\n",
    "\n",
    "# Read back in\n",
    "sdata = read_zarr(os.path.join(output_path, \"sdata.zarr\"))\n",
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make a global coordinate system\n",
    "from spatialdata.transformations import Identity\n",
    "from spatialdata.transformations import get_transformation, set_transformation\n",
    "\n",
    "transformations=get_transformation(sdata[\"REAscreen_IO_CRC_image\"], get_all=True)\n",
    "transformations[\"global\"] = Identity()\n",
    "set_transformation(sdata[\"REAscreen_IO_CRC_image\"], transformation=transformations, set_all=True, write_to_sdata=sdata)\n",
    "\n",
    "transformations=get_transformation(sdata[\"REAscreen_IO_CRC_nuclei_image\"], get_all=True)\n",
    "transformations[\"global\"] = Identity()\n",
    "set_transformation(sdata[\"REAscreen_IO_CRC_nuclei_image\"], transformation=transformations, set_all=True, write_to_sdata=sdata)\n",
    "\n",
    "# Inspect the sdata\n",
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the transformations associated with the \"REAscreen_IO_CRC_image\" image layer\n",
    "get_transformation(\n",
    "    sdata[\"REAscreen_IO_CRC_image\"],\n",
    "    get_all=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove table layers from memory and disk\n",
    "import shutil\n",
    "\n",
    "del sdata.tables['REAscreen_IO_CRC_nuclei_table']\n",
    "del sdata.tables['REAscreen_IO_CRC_table']\n",
    "\n",
    "shutil.rmtree(sdata.path / 'tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the sdata interactively\n",
    "from napari_spatialdata import Interactive\n",
    "\n",
    "# Interactive(sdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all channel names\n",
    "channels = sdata[\"REAscreen_IO_CRC_image\"][\"scale0\"][\"image\"].c.data\n",
    "channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all nuclei channel names\n",
    "nuclei_channels = sdata[\"REAscreen_IO_CRC_nuclei_image\"][\"scale0\"][\"image\"].c.data\n",
    "nuclei_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images without normalization (works for bright images such as DAPI)\n",
    "import spatialdata_plot\n",
    "\n",
    "ax=sdata.pl.render_images(\n",
    "    element=\"REAscreen_IO_CRC_image\",\n",
    "    channel=\"DAPI (1)\",\n",
    "    cmap=\"grey\",\n",
    "    scale=\"scale4\",\n",
    "    norm=None,\n",
    ").pl.show(\n",
    "    coordinate_systems=\"global\",\n",
    "    figsize=(6,6), \n",
    "    dpi=100, \n",
    "    colorbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images with normalization (to adjust the minimum and maximum visualization values)\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "vmax = 1000\n",
    "vmin = 0\n",
    "norm = Normalize(vmax=vmax, vmin=vmin, clip=True)\n",
    "\n",
    "ax=sdata.pl.render_images(\n",
    "    \"REAscreen_IO_CRC_image\",\n",
    "    channel=\"CD45\",\n",
    "    cmap=\"grey\",\n",
    "    scale=\"scale4\",\n",
    "    norm=norm,\n",
    ").pl.show(\n",
    "    coordinate_systems=\"global\",\n",
    "    figsize=(6,6), \n",
    "    dpi=100, \n",
    "    colorbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create histograms for each channel using the following function:\n",
    "hp.pl.histogram(\n",
    "    sdata,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    channel=\"CD45\",\n",
    "    bins=100,\n",
    "    fig_kwargs={\n",
    "        \"figsize\": (4, 4),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the histograms for all channels:\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Grid config\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(len(channels) / n_cols)\n",
    "\n",
    "# Specify channels to plot\n",
    "hist_channels = channels\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 3))\n",
    "axes = axes.flatten()  # Flatten to 1D array for easy indexing\n",
    "\n",
    "# Get minimum and maximum values for img dtype\n",
    "img_dtype = sdata.images[\"REAscreen_IO_CRC_image\"][\"scale0\"]['image'].dtype\n",
    "img_min = np.iinfo(img_dtype).min\n",
    "img_max = np.iinfo(img_dtype).max\n",
    "\n",
    "# Plot each histogram in its own subplot\n",
    "for i, channel in enumerate(hist_channels):\n",
    "    ax = axes[i]\n",
    "    hp.pl.histogram(\n",
    "        sdata,\n",
    "        img_layer=\"REAscreen_IO_CRC_image\",\n",
    "        channel=channel,\n",
    "        range=(img_min, img_max/2),\n",
    "        bins=100,\n",
    "        ax=ax,  # Plot into specific subplot\n",
    "    )\n",
    "    ax.set_title(channel)\n",
    "\n",
    "# Turn off any unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. QC and annotations of ROIs and artifacts in Qupath\n",
    "Qupath is open source software for bioimage analysis. It has some really nice tools for annotating regions of interest and artifacts in images. We will use these for our dataset and add the results to the SpatialData object.\n",
    "\n",
    "1. If necessary, install QUpath (v.0.5.1) using this [link](https://qupath.github.io/).\n",
    "2. Open Qupath and click `Create project` in the top left corner.\n",
    "3. Create a new empty folder for the project and click `Select Folder`.\n",
    "4. You would normally add images to a project by going to `Add images` and providing the image paths. However, this would result in the images being added separately, and we would prefer a merged image containing all channels. To do this, we will use the `read_multichannel_qupath.groovy` script. To run it, go to `Automate`, `Script Editor`, `File`, `Open`, and select the script. In the Qupath script editor, change the folderPath argument to the folder containing the MACSima images and click `Run`. This should add all images to the project as a single image.\n",
    "5. Explore the image by going to `Brightness & contrast`, select the channels you want to visualize and adjust the settings to improve the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 QC image channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Excercise</b>:\n",
    "- Create a python list of which channels you would consider successful and a list for the channels you believe failed. Since we're not experts in this tissue, we'll aim for removing the channels with obvious flaws (note that this is subjective and very dependent on how well you know the underlying biology and expected staining patterns for the biomarkers).\n",
    "\n",
    "<details> \n",
    "<summary>Click to reveal the solution</summary>\n",
    "\n",
    "```python\n",
    "successful_channels = [\n",
    "    'DAPI (1)',\n",
    "    'Bcl 2', # Clearly contains acquisition bleaching artifacts, but we can annotate these regions to remove them\n",
    "    'CD8a',\n",
    "    'FoxP3',\n",
    "    'CD138',\n",
    "    'CD15',\n",
    "    'CD45',\n",
    "    'Collagen I',\n",
    "    'CD274',\n",
    "    'PCNA',\n",
    "    'CD163',\n",
    "    'CD56',\n",
    "    'CD11b',\n",
    "    'CD45RB',\n",
    "    'CD14',\n",
    "    'CD107a',\n",
    "    'Ki 67',\n",
    "    'CD4',\n",
    "    'CD107b',\n",
    "    'CD31',\n",
    "    'CD38',\n",
    "    'CD66b',\n",
    "    'Cleaved PARP1',\n",
    "    'Mast Cell Tryptase',\n",
    "    'CD20 Cytoplasmic',\n",
    "    'Podoplanin',\n",
    "    'CD16',\n",
    "    'CD1c',\n",
    "    'Actin',\n",
    "    'CD68',\n",
    "    'Cytokeratin 20',\n",
    "    'CD79a',\n",
    "    'CD45RO',\n",
    "    'Vimentin',\n",
    "    'CD11c',\n",
    "    'CD3',\n",
    "    'CD324',\n",
    "    'HLA ABC',\n",
    "    'CD57',\n",
    "    'CD204',\n",
    "    'MART 1',\n",
    "    'Melanocyte PMEL',\n",
    "    'Cytokeratin 19',\n",
    "    'Cytokeratin',\n",
    "    'HLA DR',\n",
    "    'beta Catenin',\n",
    "    'beta Actin',\n",
    "    'CD326',\n",
    "    'Calponin',\n",
    "    'DAPI (2)',\n",
    "]\n",
    "\n",
    "failed_channels = [\n",
    "    'CD209', # Uneven illumination\n",
    "    'CD279', # Uneven illumination\n",
    "    'CD44', # Uneven illumination\n",
    "    'p53', # Uneven illumination\n",
    "    'Cytokeratin 7', # Uneven illumination\n",
    "    'CD223', # Very bright (saturating) artifact\n",
    "    'Cytokeratin 14', # Uneven illumination\n",
    "    'CD45RA', # Uneven illumination\n",
    "    'CD66b', # Very bright (saturating) artifact\n",
    "    'CD134', # Uneven illumination\n",
    "    'Cytokeratin 5', # Uneven illumination\n",
    "    'Cytokeratin 8', # Uneven illumination\n",
    "    'Cytokeratin HMW', # Uneven illumination\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_channels = [\n",
    "    'DAPI (1)',\n",
    "    'Bcl 2', # Clearly contains acquisition bleaching artifacts, but we can annotate these regions to remove them\n",
    "    'CD8a',\n",
    "    'FoxP3',\n",
    "    'CD138',\n",
    "    'CD15',\n",
    "    'CD45',\n",
    "    'Collagen I',\n",
    "    'CD274',\n",
    "    'PCNA',\n",
    "    'CD163',\n",
    "    'CD56',\n",
    "    'CD11b',\n",
    "    'CD45RB',\n",
    "    'CD14',\n",
    "    'CD107a',\n",
    "    'Ki 67',\n",
    "    'CD4',\n",
    "    'CD107b',\n",
    "    'CD31',\n",
    "    'CD38',\n",
    "    'CD66b',\n",
    "    'Cleaved PARP1',\n",
    "    'Mast Cell Tryptase',\n",
    "    'CD20 Cytoplasmic',\n",
    "    'Podoplanin',\n",
    "    'CD16',\n",
    "    'CD1c',\n",
    "    'Actin',\n",
    "    'CD68',\n",
    "    'Cytokeratin 20',\n",
    "    'CD79a',\n",
    "    'CD45RO',\n",
    "    'Vimentin',\n",
    "    'CD11c',\n",
    "    'CD3',\n",
    "    'CD324',\n",
    "    'HLA ABC',\n",
    "    'CD57',\n",
    "    'CD204',\n",
    "    'MART 1',\n",
    "    'Melanocyte PMEL',\n",
    "    'Cytokeratin 19',\n",
    "    'Cytokeratin',\n",
    "    'HLA DR',\n",
    "    'beta Catenin',\n",
    "    'beta Actin',\n",
    "    'CD326',\n",
    "    'Calponin',\n",
    "    'DAPI (2)',\n",
    "]\n",
    "\n",
    "failed_channels = [\n",
    "    'CD209', # Uneven illumination\n",
    "    'CD279', # Uneven illumination\n",
    "    'CD44', # Uneven illumination\n",
    "    'p53', # Uneven illumination\n",
    "    'Cytokeratin 7', # Uneven illumination\n",
    "    'CD223', # Very bright (saturating) artifact\n",
    "    'Cytokeratin 14', # Uneven illumination\n",
    "    'CD45RA', # Uneven illumination\n",
    "    'CD66b', # Very bright (saturating) artifact\n",
    "    'CD134', # Uneven illumination\n",
    "    'Cytokeratin 5', # Uneven illumination\n",
    "    'Cytokeratin 8', # Uneven illumination\n",
    "    'Cytokeratin HMW', # Uneven illumination\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Annotate artifacts\n",
    "\n",
    "1. Open the `Bcl 2 (C3)` channel and annotate the acquisition bleaching regions. Since these are straight lines, you can use the rectangle annotation to annotate all the segments and subsequently merge the annotation by going to `Objects`, `Annotations...`, `Merge selected`. Rename the merged annotation to `acquisition_bleaching` by right-clicking on the annotation in the `Annotations` tab and going to `Set properties`.\n",
    "\n",
    "2. Open the `DAPI (C1)` channel. In other datasets, we may find some out-of-focus regions, tissue-folds, etc that we could annotate. In this dataset, there is a small very bright region at the top right of the tissue. Annotate it using either the polygon annotation tool or the magic brush, merge any annotation objects you create and rename the merged object `bright_artifact`.\n",
    "\n",
    "3. Export the annotations. Select all artifact annotations and go to `File`, `Export objects as GeoJSON` and set to `Selected objects` and `Compression` to None. Click `OK` and set the file name to `artifacts.geojson`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Annotate ROIs\n",
    "\n",
    "1. Now, pick a region that looks of interest to you, annotate it and label it `ROI`. Select it and export the GeoJSON as `ROI.geojson.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Add annotations to sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "path_to_artifacts_GeoJSON = r'd:\\example_data_MACSima\\REAscreen_IO_CRC\\Qupath\\artifacts.geojson'\n",
    "path_to_ROI_GeoJSON = r'd:\\example_data_MACSima\\REAscreen_IO_CRC\\Qupath\\ROI.geojson'\n",
    "\n",
    "gdf_artifacts = gpd.read_file(path_to_artifacts_GeoJSON)\n",
    "sdata = hp.sh.add_shapes_layer(sdata, input=gdf_artifacts, output_layer='artifacts', overwrite=True)\n",
    "\n",
    "gdf_ROI = gpd.read_file(path_to_ROI_GeoJSON)\n",
    "sdata = hp.sh.add_shapes_layer(sdata, input=gdf_ROI, output_layer='ROI', overwrite=True)\n",
    "\n",
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect artifacts shapes layer\n",
    "sdata.shapes['artifacts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot artifacts layer\n",
    "hp.pl.plot_shapes(\n",
    "    sdata, \n",
    "    img_layer='REAscreen_IO_CRC_image', \n",
    "    channel='DAPI (1)', \n",
    "    shapes_layer='artifacts',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect ROI shapes layer\n",
    "sdata.shapes['ROI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROI layer\n",
    "hp.pl.plot_shapes(\n",
    "    sdata, \n",
    "    img_layer='REAscreen_IO_CRC_image', \n",
    "    channel='DAPI (1)', \n",
    "    shapes_layer='ROI',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will create a cropped version of the SpatialData object to speed up computation\n",
    "x_min, x_max, y_min, y_max = 7000, 10000, 2000, 5000\n",
    "\n",
    "sdata_crop = sdata.query.bounding_box(\n",
    "    min_coordinate=[x_min, y_min], max_coordinate=[x_max, y_max], axes=(\"x\", \"y\"), target_coordinate_system=\"global\"\n",
    ")\n",
    "\n",
    "# Crop shapes layers\n",
    "from shapely.geometry import box\n",
    "bbox = box(x_min, y_min, x_max, y_max)\n",
    "\n",
    "for shapes_layer in sdata_crop.shapes.keys():\n",
    "    gdf = sdata_crop.shapes[shapes_layer]\n",
    "    gdf_cropped = gdf.clip(bbox)\n",
    "    sdata_crop = hp.sh.add_shapes_layer(sdata_crop, input=gdf, output_layer=shapes_layer, overwrite=True)\n",
    "    \n",
    "# Write the SpatialData object to disk\n",
    "sdata_crop.write(os.path.join(output_path, \"sdata_crop.zarr\"))\n",
    "\n",
    "# Read back in\n",
    "sdata_crop = read_zarr(os.path.join(output_path, \"sdata_crop.zarr\"))\n",
    "sdata_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create subset of segmentation channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create a subset of channels that delineate the nucleus and cell borders for a wide variety of cell types\n",
    "segmentation_channels = [\"DAPI (1)\", \"CD8a\", \"CD15\", \"CD45\", \"PCNA\", \"CD4\", \"CD38\", \"Mast Cell Tryptase\", \"Vimentin\", \"CD11c\", \"CD3\", \"HLA ABC\", \"CD326\"]\n",
    "\n",
    "# We create a dictionary containing the index for every channel\n",
    "channel_indexes = {channel: index for index, channel in enumerate(channels)}\n",
    "\n",
    "# Now, we can get the indexes for the segmentation channels\n",
    "segmentation_channels_indexes = [channel_indexes[channel] for channel in segmentation_channels]\n",
    "segmentation_channels_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset image based on segmentation channels\n",
    "arr = sdata_crop.images[\"REAscreen_IO_CRC_image\"][\"scale0\"][\"image\"].data[segmentation_channels_indexes]\n",
    "\n",
    "# Get transformations\n",
    "from spatialdata.transformations import get_transformation\n",
    "transformations = get_transformation(sdata_crop.images[\"REAscreen_IO_CRC_image\"], get_all=True)\n",
    "\n",
    "# Add new image layer\n",
    "sdata_crop = hp.im.add_image_layer(\n",
    "    sdata_crop,\n",
    "    arr=arr,\n",
    "    output_layer=\"REAscreen_IO_CRC_image_segmentation_channels\",\n",
    "    transformations=transformations,\n",
    "    c_coords=segmentation_channels,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images based on lower and upper percentile\n",
    "sdata_crop = hp.im.normalize(\n",
    "    sdata_crop, \n",
    "    img_layer = \"REAscreen_IO_CRC_image_segmentation_channels\",\n",
    "    output_layer = \"REAscreen_IO_CRC_image_segmentation_channels_norm\",\n",
    "    q_min = 20.0,  \n",
    "    q_max = 99.9,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# hp.im.normalize gives us an image with dtype float32, but we want uint16 since clahe does not support float32 so we'll change the dtype\n",
    "img_float32 = sdata_crop.images['REAscreen_IO_CRC_image_segmentation_channels_norm'] # Values fall between 0 and 1\n",
    "img_uint16 = (img_float32 * 65535).clip(0, 65535).astype(np.uint16) # Values fall between 0 and 65535\n",
    "\n",
    "transformations = get_transformation(sdata_crop[\"REAscreen_IO_CRC_image_segmentation_channels_norm\"], get_all=True)\n",
    "\n",
    "sdata_crop = hp.im.add_image_layer(\n",
    "    sdata_crop,\n",
    "    arr=img_uint16.data,\n",
    "    output_layer=\"REAscreen_IO_CRC_image_segmentation_channels_norm\",\n",
    "    transformations=transformations,\n",
    "    c_coords=segmentation_channels,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normalized images\n",
    "hp.pl.plot_image(\n",
    "    sdata_crop,\n",
    "    img_layer = [\"REAscreen_IO_CRC_image_segmentation_channels\", \"REAscreen_IO_CRC_image_segmentation_channels_norm\"],\n",
    "    figsize = (20, 100),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform min max filtering\n",
    "sdata_crop = hp.im.min_max_filtering(\n",
    "    sdata_crop,\n",
    "    img_layer = \"REAscreen_IO_CRC_image_segmentation_channels_norm\",\n",
    "    output_layer = \"REAscreen_IO_CRC_image_segmentation_channels_minmax\",\n",
    "    size_min_max_filter = 55,\n",
    "    overwrite = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the min max filtered images\n",
    "hp.pl.plot_image(\n",
    "    sdata_crop,\n",
    "    img_layer = [\"REAscreen_IO_CRC_image_segmentation_channels_norm\", \"REAscreen_IO_CRC_image_segmentation_channels_minmax\"],\n",
    "    figsize = (20, 100),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform contrast enhancement using CLAHE\n",
    "sdata_crop = hp.im.enhance_contrast(\n",
    "    sdata_crop,\n",
    "    img_layer = \"REAscreen_IO_CRC_image_segmentation_channels_minmax\",\n",
    "    output_layer = \"REAscreen_IO_CRC_image_segmentation_channels_clahe\",\n",
    "    contrast_clip = 3.5,\n",
    "    chunks = 2048,\n",
    "    overwrite = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the contrast enhanced images\n",
    "hp.pl.plot_image(\n",
    "    sdata_crop,\n",
    "    img_layer = [\"REAscreen_IO_CRC_image_segmentation_channels_minmax\", \"REAscreen_IO_CRC_image_segmentation_channels_clahe\"],\n",
    "    figsize = (20, 100),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Nuclei and cell segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to download and unzip the Instanseg model\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import tempfile\n",
    "\n",
    "OUTPUT_DIR =  tempfile.gettempdir()\n",
    "\n",
    "def download_and_unzip(url, extract_to):\n",
    "    try:\n",
    "        os.makedirs(extract_to, exist_ok=False)\n",
    "    except FileExistsError:\n",
    "        print(\"Model already downloaded.\")\n",
    "        return\n",
    "    local_zip_path = os.path.join(extract_to, 'downloaded.zip')\n",
    "    print(\"Downloading...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(local_zip_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    print(\"Unzipping...\")\n",
    "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    os.remove(local_zip_path)\n",
    "    print(f\"Done! Files extracted to: {extract_to}\")\n",
    "\n",
    "url = \"https://github.com/instanseg/instanseg/releases/download/instanseg_models_v0.1.0/fluorescence_nuclei_and_cells.zip\"\n",
    "target_path = os.path.join(OUTPUT_DIR, \"fluorescence_nuclei_and_cells\" )\n",
    "download_and_unzip(url, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Create a local Dask cluster\n",
    "cluster = LocalCluster(\n",
    "    n_workers=4,             # Number of worker processes. Possible to increase to more workers, depending on available memory/cores\n",
    "    threads_per_worker=1,    # Number of threads per worker\n",
    "    memory_limit=\"32GB\",     # Memory limit per worker\n",
    ")\n",
    "\n",
    "# Connect a Client to the cluster\n",
    "client = Client(cluster)\n",
    "\n",
    "# Print the Dask dashboard link\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuclei segmentation using Instanseg (only on DAPI channel)\n",
    "import torch\n",
    "from instanseg import InstanSeg\n",
    "\n",
    "path_model = os.path.join(target_path, \"instanseg.pt\")\n",
    "\n",
    "instanseg_fluorescence = torch.load(path_model, weights_only=False)\n",
    "instanseg_fluorescence = InstanSeg(model_type=instanseg_fluorescence, device=\"cpu\")\n",
    "\n",
    "arr = sdata_crop.images[\"REAscreen_IO_CRC_image_segmentation_channels_clahe\"].data[[0]] # DAPI (1) has index 0\n",
    "\n",
    "from spatialdata.transformations import get_transformation\n",
    "transformations = get_transformation(sdata_crop.images[\"REAscreen_IO_CRC_image_segmentation_channels_clahe\"], get_all=True)\n",
    "\n",
    "sdata_crop = hp.im.add_image_layer(\n",
    "    sdata_crop,\n",
    "    arr=arr,\n",
    "    output_layer=\"REAscreen_IO_CRC_image_segmentation_channels_clahe_DAPI\",\n",
    "    transformations=transformations,\n",
    "    c_coords=segmentation_channels[0],\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "sdata_crop = hp.im.segment(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image_segmentation_channels_clahe_DAPI\",\n",
    "    output_labels_layer=[\"labels_nuclei_instanseg\"],\n",
    "    output_shapes_layer=[\"shapes_nuclei_instanseg\"],\n",
    "    labels_layer_align=None,\n",
    "    depth=50,\n",
    "    chunks=2048,\n",
    "    model=hp.im.instanseg_callable,\n",
    "    # parameters passed to hp.im.instanseg_callable\n",
    "    output=\"nuclei\",\n",
    "    device=\"cpu\",\n",
    "    instanseg_model=path_model,  # load it in every worker, because torchscript model is not serializable\n",
    "    pixel_size = 0.17, # pixel size in μm. Setting this correctly has a huge impact on the quality of the results.\n",
    "    iou=True,\n",
    "    trim=False,\n",
    "    to_coordinate_system=\"global\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuclei segmentation using Cellpose (only on DAPI channel)\n",
    "sdata_crop = hp.im.segment(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image_segmentation_channels_clahe_DAPI\",\n",
    "    chunks=2048,\n",
    "    depth=200,\n",
    "    model=hp.im.cellpose_callable,\n",
    "    # parameters that will be passed to the callable _cellpose:\n",
    "    pretrained_model=\"nuclei\",\n",
    "    device=\"cpu\",\n",
    "    diameter=45,\n",
    "    flow_threshold=0.9,\n",
    "    cellprob_threshold=-3,\n",
    "    output_labels_layer=\"labels_nuclei_cellpose\",\n",
    "    output_shapes_layer=\"shapes_nuclei_cellpose\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell segmentation using Instanseg (on all channels)\n",
    "sdata_crop = hp.im.segment(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image_segmentation_channels_clahe\",\n",
    "    output_labels_layer=[\"labels_cells_instanseg\"],\n",
    "    output_shapes_layer=[\"shapes_cells_instanseg\"],\n",
    "    labels_layer_align=None,\n",
    "    depth=50,\n",
    "    chunks=2048,\n",
    "    model=hp.im.instanseg_callable,\n",
    "    # parameters passed to hp.im.instanseg_callable\n",
    "    output=\"cells\",\n",
    "    device=\"cpu\",\n",
    "    instanseg_model=path_model,  # load it in every worker, because torchscript model is not serializable\n",
    "    pixel_size = 0.17, # pixel size in μm. Setting this correctly has a huge impact on the quality of the results.\n",
    "    iou=True,\n",
    "    trim=False,\n",
    "    to_coordinate_system=\"global\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.pl.plot_shapes(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image_segmentation_channels_clahe\",\n",
    "    shapes_layer=[\"shapes_nuclei_instanseg\", \"shapes_nuclei_cellpose\", \"shapes_cells_instanseg\"],\n",
    "    channel=\"DAPI (1)\",\n",
    "    alpha=1,\n",
    "    linewidth=1,\n",
    "    figsize=(30,10),\n",
    "    vmin_img=0,\n",
    "    vmax_img=65535,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Filter segmentation masks based on size and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out nuclei based on size\n",
    "pixel_size_um = 0.17\n",
    "square_pixel_size_um2 = pixel_size_um**2\n",
    "\n",
    "min_area_um2 = 10 # Minimum size of segmentation objects (in square micrometer).\n",
    "max_area_um2 = 200 # Maximum size of segmentation objects (in square micrometer).\n",
    "\n",
    "print(f\"Total number of segmentation objects: {len(sdata_crop.shapes['shapes_cells_instanseg'])}\")\n",
    "print(f\"Filtering out segmentation objects smaller than {min_area_um2} µm² and larger than {max_area_um2} µm².\")\n",
    "gdf = sdata_crop.shapes['shapes_cells_instanseg'][\n",
    "    (sdata_crop.shapes['shapes_cells_instanseg'].area >= (min_area_um2/square_pixel_size_um2)) &\n",
    "    (sdata_crop.shapes['shapes_cells_instanseg'].area <= (max_area_um2/square_pixel_size_um2))\n",
    "]\n",
    "cells_removed_size = len(sdata_crop.shapes['shapes_cells_instanseg']) - len(gdf)\n",
    "print(f\"Number of segmentation objects removed based on size: {cells_removed_size}\")\n",
    "\n",
    "sdata_crop = hp.sh.add_shapes_layer(\n",
    "    sdata_crop,\n",
    "    input = gdf,\n",
    "    output_layer = 'shapes_cells_instanseg_size_filtered',\n",
    "    overwrite = True,\n",
    ")\n",
    "\n",
    "sdata_crop = hp.im.rasterize(\n",
    "    sdata_crop,\n",
    "    shapes_layer = 'shapes_cells_instanseg_size_filtered',\n",
    "    output_layer = 'labels_cells_instanseg_size_filtered',\n",
    "    chunks = 5000,\n",
    "    overwrite = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot artifacts layer\n",
    "hp.pl.plot_shapes(\n",
    "    sdata_crop, \n",
    "    img_layer='REAscreen_IO_CRC_image_segmentation_channels_clahe', \n",
    "    channel='DAPI (1)', \n",
    "    shapes_layer='artifacts',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out segmentation masks in artifacts\n",
    "print(f'Filtering out segmentation objects in artifacts annotations.')\n",
    "cells_gdf = sdata_crop.shapes['shapes_cells_instanseg_size_filtered']\n",
    "artifacts_gdf = sdata_crop.shapes['artifacts']\n",
    "\n",
    "# Identify cells whose geometry intersects the artifacts\n",
    "cells_gdf['overlaps_artifact'] = cells_gdf.geometry.intersects(artifacts_gdf.unary_union)\n",
    "filtered_cells_gdf = cells_gdf[~cells_gdf['overlaps_artifact']].copy()\n",
    "filtered_cells_gdf.drop(columns=['overlaps_artifact'], inplace=True)\n",
    "\n",
    "cells_removed_artifacts = len(cells_gdf) - len(filtered_cells_gdf)\n",
    "print(f\"Number of segmentation objects removed from artifact annotations: {cells_removed_artifacts}\")\n",
    "\n",
    "sdata_crop = hp.sh.add_shapes_layer(\n",
    "    sdata_crop, \n",
    "    input = filtered_cells_gdf,\n",
    "    output_layer = 'shapes_cells_instanseg_artifacts_filtered',\n",
    "    overwrite = True\n",
    ")\n",
    "\n",
    "sdata_crop = hp.im.rasterize(\n",
    "    sdata_crop,\n",
    "    shapes_layer = 'shapes_cells_instanseg_artifacts_filtered',\n",
    "    output_layer = 'labels_cells_instanseg_artifacts_filtered',\n",
    "    chunks = 5000,\n",
    "    overwrite = True,\n",
    ")\n",
    "\n",
    "# FIXME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.pl.plot_shapes(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image_segmentation_channels_clahe\",\n",
    "    shapes_layer=[\"shapes_cells_instanseg\", \"shapes_cells_instanseg_size_filtered\", \"shapes_cells_instanseg_artifacts_filtered\"],\n",
    "    channel=\"DAPI (1)\",\n",
    "    alpha=1,\n",
    "    linewidth=1,\n",
    "    figsize=(30,10),\n",
    "    vmin_img=0,\n",
    "    vmax_img=65535,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Align nucleus and cell segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the nucleus and cell segmentations (we'll use the cellpose nucleus segmentation since it outperformed Instanseg)\n",
    "sdata_crop=hp.im.align_labels_layers( \n",
    "    sdata_crop,\n",
    "    labels_layer_1=\"labels_nuclei_cellpose\",\n",
    "    labels_layer_2=\"labels_cells_instanseg_artifacts_filtered\",\n",
    "    output_labels_layer=\"labels_nuclei_cellpose_aligned\",\n",
    "    output_shapes_layer=\"shapes_nuclei_cellpose_aligned\",\n",
    "    chunks=2048,\n",
    "    depth=300, # Make sure to set this to a number larger than the approx. cell size to avoid chunking effects.\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# This function aligns two label layers by examining the labels in labels_layer_1 and identifying their maximum overlap with labels in labels_layer_2.\n",
    "# It then updates the labels of labels_layer_1 (i.e. \"labels_nuclei_cellpose\").\n",
    "# If a label from labels_layer_1 has no overlap with a label from label_layer_2, the label in labels_layer_1 is set to zero (i.e. background).\n",
    "# If two labels from labels_layer_1 overlap with the same label from labels_layer_2, they are effectively merged into the same new label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "hp.pl.sanity(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image_segmentation_channels_clahe\",\n",
    "    shapes_layer=\"shapes_nuclei_cellpose\",\n",
    "    points_layer=None,\n",
    "    crd=[7000, 7400, 3500, 3900],\n",
    "    plot_cell_number=True,\n",
    "    figsize=(8,8)\n",
    ")\n",
    "\n",
    "hp.pl.sanity(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image_segmentation_channels_clahe\",\n",
    "    shapes_layer=\"shapes_cells_instanseg_artifacts_filtered\",\n",
    "    points_layer=None,\n",
    "    crd=[7000, 7400, 3500, 3900],\n",
    "    plot_cell_number=True,\n",
    "    figsize=(8,8),\n",
    ")\n",
    "\n",
    "hp.pl.sanity(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image_segmentation_channels_clahe\",\n",
    "    shapes_layer=\"shapes_nuclei_cellpose_aligned\",\n",
    "    points_layer=None,\n",
    "    crd=[7000, 7400, 3500, 3900],\n",
    "    plot_cell_number=True,\n",
    "    figsize=(8,8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cell properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cell intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate intensities for cell segmentations.\n",
    "marker_channels = [channel for channel in successful_channels if channel not in nuclei_channels]\n",
    "\n",
    "sdata_crop = hp.tb.allocate_intensity( \n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    labels_layer=\"labels_cells_instanseg_artifacts_filtered\",\n",
    "    output_layer=\"table_cells\",\n",
    "    channels=marker_channels,\n",
    "    mode=\"sum\", # When set to \"sum\", the total intensity for each channel will be added to `.X` of the table layer; if set to `\"mean\"`, it calculates the average intensity per channel.\n",
    "    # obs_stats=[\"sum\", \"mean\", \"count\", \"var\", \"kurtosis\", \"skew\", \"max\", \"min\"] # Stats that will be added to `.obs` of `output_layer`\n",
    "    chunks = 2048,\n",
    "    to_coordinate_system=\"global\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect new table layer\n",
    "sdata_crop.tables[\"table_cells\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect X\n",
    "sdata_crop.tables[\"table_cells\"].to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect obs\n",
    "sdata_crop.tables[\"table_cells\"].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate intensities for nucleus segmentations.\n",
    "sdata_crop = hp.tb.allocate_intensity( \n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    labels_layer=\"labels_nuclei_cellpose_aligned\",\n",
    "    output_layer=\"table_nucleus\",\n",
    "    channels=marker_channels,\n",
    "    mode=\"sum\", # When set to \"sum\", the total intensity for each channel will be added to `.X` of the table layer; if set to `\"mean\"`, it calculates the average intensity per channel.\n",
    "    # obs_stats=[\"sum\", \"mean\", \"count\", \"var\", \"kurtosis\", \"skew\", \"max\", \"min\"] # Stats that will be added to `.obs` of `output_layer`\n",
    "    chunks = 2048,\n",
    "    to_coordinate_system=\"global\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Morphological properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add morphological properties for cell\n",
    "sdata_crop = hp.tb.add_regionprop_features(\n",
    "    sdata_crop, \n",
    "    labels_layer='labels_cells_instanseg_artifacts_filtered', \n",
    "    table_layer='table_cells'\n",
    ")\n",
    "\n",
    "# area: Area of the segmentation object (in pixels).\n",
    "# eccentricity: A measure for how much the segmentation object deviates from being circular. Values are between 0-1 (a perfect circle would have value 0).\n",
    "# major_axis_length: The length of the longest axis of the segmentation object (in pixels).\n",
    "# minor_axis_length: The length of the shortest axis of the segmentation object (in pixels).\n",
    "# perimeter: The perimeter of the segmentation object (in pixels).\n",
    "# centroid_x: x-coordinate of the centroid of the segmentation object (in pixels)\n",
    "# centroid_y: y-coordinate of the centroid of the segmentation object (in pixels)\n",
    "# convex_area: Area of the convex hull image, which is the smallest convex polygon that encloses the region (in pixels).\n",
    "# equivalent_diameter: The diameter of a circle with the same area as the segmentation object (in pixels).\n",
    "# _major_minor_axis_ratio: The ratio of the major_axis_length over the minor_axis_length. \n",
    "# _perim_square_over_area: The ratio of the square of the perimeter over the area.\n",
    "# _major_axis_equiv_diam: major_axis_length / equivalent_diameter.\n",
    "# _convex_hull_resid: (convex_area - area) / convex_area.\n",
    "# _centroid_dif: the normalized euclidean distance between the segmentation object centroid and the corresponding convex hull centroid.\n",
    "# NOTE: While everything is calculated in pixels, later in this notebook, we will convert those values into micrometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect obs\n",
    "sdata_crop.tables[\"table_cells\"].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add morphological properties for nuclei\n",
    "sdata_crop = hp.tb.add_regionprop_features(\n",
    "    sdata_crop, \n",
    "    labels_layer='labels_nuclei_cellpose_aligned', \n",
    "    table_layer='table_nucleus'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect obs\n",
    "sdata_crop.tables[\"table_nucleus\"].obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Clean and merge nucleus and cell tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean table layers\n",
    "import pandas as pd\n",
    "from harpy.utils._keys import _INSTANCE_KEY, _REGION_KEY\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "pixel_size_um = 0.17\n",
    "square_pixel_size_um2 = pixel_size_um**2\n",
    "\n",
    "def clean_table_layer(sdata, table_layer, suffix, channels, pixel_size_um, square_pixel_size_um2):   \n",
    "    # Create copy of table layer\n",
    "    adata = sdata.tables[table_layer].copy()\n",
    "    \n",
    "    # Add columns with total intensities to obs\n",
    "    intensity_df = adata.to_df()\n",
    "    adata.obs = pd.merge(adata.obs, intensity_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Correct intensities for area\n",
    "    for channel in channels:\n",
    "        adata.obs[f'{channel}_average_intensity'] = adata.obs[f'{channel}'] / adata.obs['area']\n",
    "            \n",
    "    # Drop columns with total intensities\n",
    "    adata.obs = adata.obs.drop(columns=channels)\n",
    "\n",
    "    # Sanitize column names (obs column names must contain only alphanumeric characters, underscores, dots and hyphens)\n",
    "    def sanitize_column_names(df):\n",
    "        df.columns = [\n",
    "            re.sub(r\"[^\\w\\.-]\", \"_\", col)  # Only allow [a-zA-Z0-9_.-], replace others with underscore\n",
    "            for col in df.columns\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "    adata.obs = sanitize_column_names(adata.obs)\n",
    "\n",
    "    # Clean sdata.table.obs and convert pixel measurements to micrometer\n",
    "    area_columns = [\"area\", \"convex_area\"]\n",
    "    length_columns = [\"major_axis_length\", \"minor_axis_length\", \"perimeter\", \"equivalent_diameter\"]\n",
    "\n",
    "    adata.obs = (\n",
    "        adata.obs\n",
    "        # Convert area columns from pixels squared to micrometers squared\n",
    "        .apply(lambda x: x * square_pixel_size_um2 if x.name in area_columns else x)\n",
    "        # Convert length columns from pixels to micrometers\n",
    "        .apply(lambda x: x * pixel_size_um if x.name in length_columns else x)\n",
    "        # Strip underscores from column names\n",
    "        .rename(columns=lambda x: x.lstrip('_'))\n",
    "        # Change names of centroid columns\n",
    "        .rename(columns={\n",
    "                'centroid-0': 'centroid_y',\n",
    "                'centroid-1': 'centroid_x',\n",
    "        })\n",
    "        # Add suffixes to columns\n",
    "        .rename(columns=lambda x: f'{x}_{suffix}' if x not in [_INSTANCE_KEY, _REGION_KEY] else x)\n",
    "    )\n",
    "\n",
    "    return adata   \n",
    "\n",
    "adata_nucleus = clean_table_layer(sdata_crop, 'table_nucleus', 'nucleus', marker_channels, pixel_size_um, square_pixel_size_um2)\n",
    "adata_cells = clean_table_layer(sdata_crop, 'table_cells', 'cells', marker_channels, pixel_size_um, square_pixel_size_um2)\n",
    "\n",
    "# Merge adata\n",
    "obs_nucleus = adata_nucleus.obs.drop(columns=[_REGION_KEY]).reset_index(drop=True) # Remove region key\n",
    "cells_obs = adata_cells.obs.reset_index()\n",
    "adata_cells.obs = pd.merge(cells_obs, obs_nucleus, on='cell_ID', how='left', suffixes=('', '_nucleus'))\n",
    "\n",
    "# Add column that checks whether the cell has a nucleus\n",
    "adata_cells.obs['contains_nucleus'] = ~adata_cells.obs['centroid_x_nucleus'].isna()\n",
    "\n",
    "# Add merged table back to sdata\n",
    "sdata_crop = hp.tb.add_table_layer(\n",
    "    sdata_crop,\n",
    "    adata=adata_cells,\n",
    "    output_layer='table_intensities',\n",
    "    region=adata_cells.obs[_REGION_KEY].cat.categories.to_list(),\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Remove table layers from memory and disk\n",
    "del sdata_crop.tables['table_nucleus']\n",
    "del sdata_crop.tables['table_cells']\n",
    "\n",
    "shutil.rmtree(sdata_crop.path / 'tables' / 'table_nucleus')\n",
    "shutil.rmtree(sdata_crop.path / 'tables' / 'table_cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect table\n",
    "sdata_crop.tables['table_intensities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect table obs\n",
    "sdata_crop.tables['table_intensities'].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot which cells have nuclei\n",
    "hp.pl.plot_shapes( \n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    table_layer=\"table_intensities\",\n",
    "    shapes_layer=\"shapes_cells_instanseg_artifacts_filtered\",\n",
    "    column=\"contains_nucleus\",\n",
    "    cmap='Set1',\n",
    "    channel=\"DAPI (1)\",\n",
    "    linewidth=0.2,\n",
    "    alpha=0.7,\n",
    "    figsize=(8,8),\n",
    "    to_coordinate_system=\"global\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cell density using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Get cell coordinates\n",
    "df_cells = pd.DataFrame(sdata_crop.tables[\"table_intensities\"].obsm['spatial'], columns=['x', 'y'])\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot cell density\n",
    "h1 = ax.hexbin(\n",
    "    df_cells[\"x\"], df_cells[\"y\"],\n",
    "    gridsize=30,\n",
    "    cmap=\"viridis\",\n",
    "    linewidths=0.2,\n",
    "    edgecolors='face',\n",
    ")\n",
    "fig.colorbar(h1, ax=ax, label='Cell Count')\n",
    "ax.set_title(\"Cell Density (Hexbin)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.axis(\"equal\")\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata_crop = hp.tb.preprocess_proteomics(\n",
    "    sdata_crop,\n",
    "    labels_layer=\"labels_cells_instanseg_artifacts_filtered\",\n",
    "    table_layer=\"table_intensities\",\n",
    "    output_layer=\"table_intensities_preprocessed\",\n",
    "    size_norm=True,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Leiden clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Leiden clustering\n",
    "sdata_crop = hp.tb.leiden(\n",
    "    sdata_crop,\n",
    "    labels_layer=\"labels_cells_instanseg_artifacts_filtered\",\n",
    "    table_layer=\"table_intensities_preprocessed\",\n",
    "    output_layer=\"table_intensities_leiden\",\n",
    "    calculate_umap=True,\n",
    "    calculate_neighbors=True,\n",
    "    n_pcs=17, # The number of principal components to use when calculating neighbors.\n",
    "    n_neighbors=35, # The number of neighbors to consider when calculating neighbors.\n",
    "    resolution=0.4,\n",
    "    rank_genes=True,\n",
    "    key_added=\"leiden\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Plot UMAP\n",
    "sc.pl.umap(sdata_crop.tables[\"table_intensities_leiden\"], color=[\"leiden\"], show=True)\n",
    "\n",
    "# Plot rank plot\n",
    "sc.pl.rank_genes_groups(\n",
    "    sdata_crop.tables[\"table_intensities_leiden\"],\n",
    "    n_genes=8,\n",
    "    sharey=False,\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.pl.plot_shapes( \n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    table_layer=\"table_intensities_leiden\",\n",
    "    shapes_layer=\"shapes_cells_instanseg_artifacts_filtered\",\n",
    "    column=\"leiden\",\n",
    "    channel=\"DAPI (1)\",\n",
    "    linewidth=0.2,\n",
    "    alpha=0.7,\n",
    "    figsize=(8,8),\n",
    "    to_coordinate_system=\"global\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot other variables on the UMAP as well\n",
    "from matplotlib.pyplot import rc_context\n",
    "color_vars = [\n",
    "    \"area_cells\",\n",
    "    \"contains_nucleus\",\n",
    "    \"leiden\",\n",
    "    \"CD11c\",\n",
    "    \"CD326\",\n",
    "    \"Actin\",\n",
    "]\n",
    "with rc_context({\"figure.figsize\": (3, 3)}):\n",
    "    sc.pl.umap(sdata_crop.tables[\"table_intensities_leiden\"], color=color_vars, ncols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix plot\n",
    "sc.pl.matrixplot(\n",
    "    sdata_crop.tables[\"table_intensities_leiden\"], \n",
    "    var_names=marker_channels, \n",
    "    groupby=\"leiden\", \n",
    "    cmap=\"Blues\",\n",
    "    standard_scale=\"var\",\n",
    "    colorbar_title=\"column scaled\\nexpression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ilastik object classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ilastik.sdata_to_ilastik import export_h5, add_ilastik_to_sdata, assign_ilastik_cell_types, combine_ilastik_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Export images for Ilastik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to export h5 files\n",
    "img_layer_to_export = \"REAscreen_IO_CRC_image\"\n",
    "path_to_raw_h5 = os.path.join(output_path, f'raw_images.h5')\n",
    "\n",
    "segmentation_mask_to_export = \"labels_cells_instanseg_artifacts_filtered\"\n",
    "path_to_segmentation_h5 = os.path.join(output_path, 'segmentation_mask.h5')\n",
    "\n",
    "# Export raw image channels\n",
    "export_h5(\n",
    "    sdata=sdata_crop, \n",
    "    img_layer=img_layer_to_export, \n",
    "    channels = None, # Will export all channels\n",
    "    output=path_to_raw_h5, \n",
    "    crd=None # Images will not be cropped\n",
    ")\n",
    "\n",
    "# Export segmentation mask\n",
    "export_h5(\n",
    "    sdata=sdata_crop, \n",
    "    labels_layer=segmentation_mask_to_export, \n",
    "    output=path_to_segmentation_h5, \n",
    "    crd=None\n",
    ")\n",
    "\n",
    "# NOTE: To avoid crashing Ilastik, the images should not be too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Ilastik object classifiers\n",
    "\n",
    "There are a variety of diffent object classifiers that can be created in ilastik.\n",
    "\n",
    "Some options:\n",
    "- You can train on a single image or on a combination of images (but more images will increase complexity and computation time)\n",
    "- You can train on any set of features that is most appropriate (but more features will increase complexity and computation time)\n",
    "- You can have as many classes as is needed (but more classes will increase complexity and computation time)\n",
    "- As a rule of thumb, you can classify for anything as long as you can visually recognize it yourself on an image (or a set of images)\n",
    "\n",
    "For example:\n",
    "- You can use the nucleus channel to classify cells in good segmentations and bad segmentations.\n",
    "- You can train for each channel a classifier that classifies cells in positive and negative for that marker.\n",
    "- You can train on multiple channels simultaneously to classify cells in different cell types.\n",
    "\n",
    "For each object classifier, do the following:\n",
    "\n",
    "Open Ilastik (v.1.4.0) and create an new project: Object Classification [Inputs: Raw Data, Segmentation]\n",
    "\n",
    "Note that this documentation was written for version v.1.4.0, but everything seems to work with the latest version (v.1.4.1) as well.\n",
    "\n",
    "**1. Input Data** </br>\n",
    "To load in a separate channel, you need to do the following:\n",
    "\n",
    "- Under the tab `Raw Data` from `1. Input Data`, you would click `Add...` and select `Add separate Image(s)...`.\n",
    "- Select the .h5 file containing the raw images and you will need to specify which channel you want to work on from the drop-down menu and click `OK` (this specifies the internal path in the h5 file).\n",
    "\n",
    "To load in multiple channels, you need to specify a correct pattern that also includes the internal path (i.e. in the h5 file) to the correct images.\n",
    "\n",
    "For example, to combine channel_1 and channel_2, you would do the following steps:\n",
    "- Under the tab 'Raw Data' from '1. Input Data', you would click 'Add...' and select 'Add a single 3D/4D Volume from Sequence...'.\n",
    "- Under `Specify Pattern`, you enter a patterns that specifies the images of interest. For example: `/path/to/images/raw_images.h5/channel_1; /path/to/images/raw_images.h5/channel_2` and click `Apply`. It is important that the path is specified correctly, multiple paths should be separated by a semicolon and the internal path in the h5 file should be specified as well in the path.\n",
    "- Select `Stack Across: C` before clicking `OK`.\n",
    "\n",
    "You will also need to add the segmentation mask. Under the tab `Segmentation Image`, you click `Add...` and select `Add separate Image(s)...`. You then select the appropriate segmentation mask file (e.g. `segmentation_mask.h5`).\n",
    "\n",
    "After adding the Raw Data and the Segmentation Image to the ilastik project, it is useful to right-click on them, go to `Edit properties...` and make sure `Storage:` is set to `Copy into project file`. This makes sure, you can move around the ilastik project file (on your computer or even to other computers) without losing the link to the files the project was trained on (and risk losing your training). It is also useful to set `Nickname:` to something informative such as the name of the tissue/sample/replicate/etc (to keep track of which image is which). By default, this will be set to the filename.\n",
    "\n",
    "<b>Important:</b> Note that the images for training in the Ilastik project can't be too large or Ilastik will crash. Around 10000 x 10000 pixels seems to work fine, but smaller (for training at least) is preferred. \n",
    "\n",
    "**2. Object Feature Selection** </br>\n",
    "To select features for training, it would be recommended to not select all of them, but be mindful of which you want to train on. Although ilastik itself describes computing many features at once as computationally cheap, it can still really add up to calculate all features if there are a lot of cells in each image. Additionally, by, for example, only working on intensity-related features, it becomes more explainable what the model was trained on how it can be interpreted.\n",
    "\n",
    "For most cases, we would recommend to follow these steps:\n",
    "- Under the tab `2. Object Feature Selection`, click `Select Features` and click all boxes under `Intensity Distribution`. You can add other features that you know are relevant, if needed (such as `Size in pixels` or `Diameter`). In case you want create a classifier to distinguish good segmentations from bad segmentation, it would make sense to select all features.\n",
    "- After clicking `OK`, wait until all features have been computed before moving on to the next step.\n",
    "\n",
    "<b>Important:</b> Adding too many features (i.e. more than is necessary to get good classification) risks crashing the Ilastik project. This can also create problems for large images when running headless mode. Working on only the intensity features seems to work fine.\n",
    "\n",
    "**3. Object Classification** </br>\n",
    "Here, you can create multiple classes for your classifier and train them until you are satisfied with the results. In general, it is useful to initially add a good amount of labels for the different classes for different regions of the image (or even already over multiple images) to capture the variation that is in the data and click `Live Update` to see the prediction results. Subsequently, you can focus more on the mistakes that are being made and add labels to correct those. When labeling, we suggest to unclick `Live Update` to avoid waiting time. It can be useful to use the `Uncertainty` layer to see which objects are still not robustly trained for. When training, ilastik does not make it easy to change the visualization, but if you right-click on `Raw Input` and select `Adjust Thresholds`, you have some options to change the display. When training on individual channels for positive/negative, preferably use the following format to name the classes: channel_pos and channel_neg (e.g.: CD45_pos and CD45_neg).\n",
    "\n",
    "Note that you can save your training labels by right-clicking `Labels` and subsequently `Export...`. Set the `File` to where you want to save the labels and under what name. This entire process needs to be done for every image separately, but it can be worth it to be able to recreate the ilastik project if something goes wrong or if we would want to preprocess the images for example. Of course, this only works with the exact same segmentation masks (so not after changing segmentation settings or working on a different image crop).\n",
    "\n",
    "**4. Object Information Export** </br>\n",
    "<b>Important</b>: You need to click `Configure Feature Table Export` and change `Format` to `CSV (.csv)`. You also need to set these settings when you don't want to export the results from the GUI, but will use headless mode in, for example, a Jupyter notebook. The headless mode requires this setting to be set in the GUI so if you don't do it, you will get an error later on. \n",
    "\n",
    "If you do want to export the results for the training data, you can specify under `Choose File` where and under what name you want to export the results. By default, this is set to `{dataset_dir}/{nickname}.csv`, with `{dataset_dir}` refering to the directory the raw_images.h5 file is in and `{nickname}` refering to the name specified in the `1. Input Data` tab. Preferably, remove `{nickname}` and put a unique name in its place for each ilastik classifier. For example: for a classifier to label cells as positive/negative for channel_1, put `{dataset_dir}/channel_1.csv`.\n",
    "\n",
    "Note that, by default, the object predictions will be saved as images as well, while these files will not be used in the subsequent analysis steps (since we will get all useful data from the csv files). Unfortunately, there is no way to avoid saving these files.\n",
    "\n",
    "**5. Batch processing** </br>\n",
    "We will not use the batch processing tab since it is not possible to specify the internal paths in the h5 files.\n",
    "\n",
    "**6. Blockwise Object classification** </br>\n",
    "<b>Important</b>: Before closing the project, you need to set the Blocks to, for example, 4000 x 4000 and the Halo to 100 x 100 and save the project. This is necessary for running the headless mode since it gets these settings from the Ilastik project and you can't set the settings in headless mode itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify paths for Ilastik object classifiers\n",
    "ilastik_obj_classifiers = {\n",
    "    'CD8a': { # Name of Ilastik object classifier. This will be used as an identifier for the classifier throughout the analysis \n",
    "        'path': r'd:\\example_data_MACSima\\REAscreen_IO_CRC\\output_harpy\\CD8a.ilp', # Path to Ilastik object classifier.\n",
    "        'raw_images_internal_path_list': ['CD8a'], # Internal path in h5 file that was used during Ilastik training.\n",
    "        'csv_path': output_path, # Path to where you manually exported the CSV from the GUI, or where you want the headless mode to export the CSV to.\n",
    "        'headless': False # Whether to use headless mode to creat the CSV\n",
    "    },\n",
    "    'CD3': { # Name of Ilastik object classifier. This will be used as an identifier for the classifier throughout the analysis \n",
    "        'path': r'd:\\example_data_MACSima\\REAscreen_IO_CRC\\output_harpy\\CD3.ilp', # Path to Ilastik object classifier.\n",
    "        'raw_images_internal_path_list': ['CD3'], # Internal path in h5 file that was used during Ilastik training.\n",
    "        'csv_path': output_path, # Path to where you manually exported the CSV from the GUI, or where you want the headless mode to export the CSV to.\n",
    "        'headless': False # Whether to use headless mode to creat the CSV\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 OPTIONAL: Creating CSV-files using Ilastik headless mode\n",
    "This is a useful option if you created an object classifier and you want to use it with data that is not in the training set. Headless mode will then use the existing model and create a CSV-file for the new dataset.\n",
    "\n",
    "Headless mode is also very useful if your image is too large to reliably work with the Ilastik GUI. You can then train the classifier on a crop of the data and use headless mode to get the results for the entire image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Specify path to Ilastik installation\n",
    "path_to_ilastik_exe = r'c:\\Program Files\\ilastik-1.4.1.post1\\ilastik.exe' # NOTE: Set this to the path to your Ilastik installation\n",
    "\n",
    "# Run through all classifiers\n",
    "for classifier_name, classifier_dict in ilastik_obj_classifiers.items():\n",
    "    \n",
    "    classifier_path = classifier_dict['path']\n",
    "    raw_images_internal_path_list = classifier_dict['raw_images_internal_path_list']\n",
    "    csv_path = classifier_dict['csv_path']\n",
    "    headless = classifier_dict['headless']\n",
    "    \n",
    "    # Skip classifier if headless == False\n",
    "    if not headless:\n",
    "        print(f'skipping: {classifier_name}')\n",
    "        continue\n",
    "    \n",
    "    # Expected output CSV filename (with '_table.csv' suffix)\n",
    "    actual_csv_filename = os.path.join(csv_path, classifier_name + '_table.csv') # This is different from the '--table_filename' path because Ilastik always adds '_table' to the filename.\n",
    "    \n",
    "    # Check if output file already exists (possibly from exporting training data from the Ilastik GUI)\n",
    "    if os.path.exists(actual_csv_filename):\n",
    "        raise FileExistsError(f\"{actual_csv_filename} already exists.\")\n",
    "    \n",
    "    # Run ilastik headless as a subprocess\n",
    "    cmd = [\n",
    "        path_to_ilastik_exe,\n",
    "        '--headless',\n",
    "        '--project=' + classifier_path,\n",
    "        '--raw_data=' + \";\".join([f\"{path_to_raw_h5}/{channel}\" for channel in raw_images_internal_path_list]),\n",
    "        '--segmentation_image=' + path_to_segmentation_h5 + segmentation_mask_to_export,\n",
    "        '--output_format=' + 'png',\n",
    "        '--export_source=' + \"Blockwise Object Predictions\", # Block size has to be set in the Ilastik project. We recommmend Blocks of 4000x4000 pixel and a Halo of 100x100 pixels.\n",
    "        '--table_filename=' + os.path.join(csv_path, classifier_name + '.csv') # CSV has to be set as the export format in the Ilastik project.\n",
    "    ]\n",
    "\n",
    "    print(f'Running: {classifier_name}')\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "    ## OPTIONAL: Print the output and errors for debugging\n",
    "    print('stdout:', result.stdout)\n",
    "    print('stderr:', result.stderr)\n",
    "\n",
    "    # Check if CSV file has been created\n",
    "    if os.path.exists(actual_csv_filename):\n",
    "        print(f\"Output CSV file {actual_csv_filename} has been created successfully.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Output CSV file {actual_csv_filename} was not created.\")\n",
    "    \n",
    "# NOTE: \n",
    "#  - When running this cell, make sure to close the Ilastik GUI since the headless mode will give an error if one of the ilastik projects is opened.\n",
    "#  - Ilastik is very optimistic and will always tell you \"The operation completed successfully.\", even when there was an error. This is why we check whether the file was actually created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Adding Ilastik data back to SpatialData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data from all ilastik csv files in folder to sdata\n",
    "for classifier_name, classifier_dict in ilastik_obj_classifiers.items():\n",
    "    \n",
    "    # Get path to CSV\n",
    "    csv_path = classifier_dict['csv_path']\n",
    "    actual_csv_filename = os.path.join(csv_path, classifier_name + '_table.csv')\n",
    "    \n",
    "    # Add ilastik results to sdata\n",
    "    print('Running: ', classifier_name)\n",
    "    add_ilastik_to_sdata(\n",
    "        sdata = sdata_crop,\n",
    "        input_path = actual_csv_filename,\n",
    "        table_layer = 'table_intensities_leiden',\n",
    "        labels_layer = segmentation_mask_to_export,\n",
    "        centroid_column_x = 'centroid_x_cells',\n",
    "        centroid_column_y = 'centroid_y_cells', \n",
    "        suffix = classifier_name\n",
    "    )\n",
    "\n",
    "# NOTE: This code will attempt to merge the data obtained from the ilastik classifiers to the sdata.tables[table_layer].obs based on the centroid coordinates of the sdata cells and the ilastik objects (i.e. for each cell in the sdata, the closest cell in the ilastik data will be considered a match). \n",
    "# The suffix parameter is used to add a unique identifier to all columns associated with each ilastik dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect table layer obs\n",
    "sdata_crop.tables[\"table_intensities_leiden\"].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot user labels\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap([\"#0082C8\", \"#848484\", \"#FFE119\"])\n",
    "\n",
    "hp.pl.plot_shapes(\n",
    "    sdata_crop, \n",
    "    img_layer='REAscreen_IO_CRC_image', \n",
    "    channel='DAPI (1)', \n",
    "    shapes_layer='shapes_cells_instanseg_artifacts_filtered',\n",
    "    table_layer='table_intensities_leiden',\n",
    "    alpha=1,\n",
    "    linewidth=0.6,\n",
    "    cmap=cmap,\n",
    "    column=\"ilastik_user_label_CD8a\",\n",
    ")\n",
    "# There will be no user labels if you use headless mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "cmap = ListedColormap(['#0082C8', \"#FFE119\"])\n",
    "\n",
    "hp.pl.plot_shapes(\n",
    "    sdata_crop, \n",
    "    img_layer='REAscreen_IO_CRC_image', \n",
    "    channel='DAPI (1)', \n",
    "    shapes_layer='shapes_cells_instanseg_artifacts_filtered',\n",
    "    table_layer='table_intensities_leiden',\n",
    "    alpha=1,\n",
    "    linewidth=0.6,\n",
    "    cmap=cmap,\n",
    "    column=\"ilastik_predicted_class_CD8a\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot probabilities\n",
    "hp.pl.plot_shapes(\n",
    "    sdata_crop, \n",
    "    img_layer='REAscreen_IO_CRC_image', \n",
    "    channel='DAPI (1)', \n",
    "    shapes_layer='shapes_cells_instanseg_artifacts_filtered',\n",
    "    table_layer='table_intensities_leiden',\n",
    "    alpha=1,\n",
    "    linewidth=0.6,\n",
    "    cmap='coolwarm',\n",
    "    column=\"ilastik_probability_pos_CD8a\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot probabilities\n",
    "hp.pl.plot_shapes(\n",
    "    sdata_crop, \n",
    "    img_layer='REAscreen_IO_CRC_image', \n",
    "    channel='DAPI (1)', \n",
    "    shapes_layer='shapes_cells_instanseg_artifacts_filtered',\n",
    "    table_layer='table_intensities_leiden',\n",
    "    alpha=1,\n",
    "    linewidth=0.6,\n",
    "    cmap='coolwarm',\n",
    "    column=\"ilastik_probability_neg_CD8a\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Assigning cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_table_path = os.path.join(output_path, \"annotation_matrix.csv\")\n",
    "\n",
    "sdata_crop = assign_ilastik_cell_types(\n",
    "    sdata_crop, \n",
    "    annotation_table_path=annotation_table_path,\n",
    "    table_layer='table_intensities_leiden',\n",
    "    labels_layer='labels_cells_instanseg_artifacts_filtered', # The label layer that corresponds to the data in the table layer.\n",
    "    output_column='ilastik_cell_types', # Name of output column\n",
    "    default_value='other', # Values used for all cells that do not fit any of the conditions.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cell types\n",
    "hp.pl.plot_shapes(\n",
    "    sdata_crop, \n",
    "    img_layer='REAscreen_IO_CRC_image', \n",
    "    channel='DAPI (1)', \n",
    "    shapes_layer='shapes_cells_instanseg_artifacts_filtered',\n",
    "    table_layer='table_intensities_leiden',\n",
    "    alpha=1,\n",
    "    linewidth=0.6,\n",
    "    cmap='rainbow',\n",
    "    column=\"ilastik_cell_types\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ilastik pixel classification\n",
    "Creating pixel classifiers in Ilastik can be useful to semi-automatically annotate ROIs/artifacts/tissue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Train pixel classifier\n",
    "Open Ilastik (v.1.4.0) and create an new project: Pixel Classification.\n",
    "\n",
    "Note that this documentation was written for version v.1.4.0, but everything seems to work with the latest version (v.1.4.1) as well.\n",
    "\n",
    "**1. Input Data** </br> \n",
    "To load in a separate channel, you need to do the following:\n",
    "\n",
    "- Under the tab `Raw Data` from `1. Input Data`, you would click `Add...` and select `Add separate Image(s)...`.\n",
    "- Select the .h5 file containing the raw images and you will need to specify which channel you want to work on from the drop-down menu and click `OK` (this specifies the internal path in the h5 file).\n",
    "\n",
    "To load in multiple channels, you need to specify a correct pattern that also includes the internal path (i.e. in the h5 file) to the correct images.\n",
    "\n",
    "For example, to combine channel_1 and channel_2, you would do the following steps:\n",
    "- Under the tab 'Raw Data' from '1. Input Data', you would click 'Add...' and select 'Add a single 3D/4D Volume from Sequence...'.\n",
    "- Under `Specify Pattern`, you enter a patterns that specifies the images of interest. For example: `/path/to/images/raw_images.h5/channel_1; /path/to/images/raw_images.h5/channel_2` and click `Apply`. It is important that the path is specified correctly, multiple paths should be separated by a semicolon and the internal path in the h5 file should be specified as well in the path.\n",
    "- Select `Stack Across: C` before clicking `OK`.\n",
    "\n",
    "After adding the Raw Data to the ilastik project, it is useful to right-click on them, go to `Edit properties...` and make sure `Storage:` is set to `Copy into project file`. This makes sure, you can move around the ilastik project file (on your computer or even to other computers) without losing the link to the files the project was trained on (and risk losing your training). It is also useful to set `Nickname:` to something informative such as the name of the tissue/sample/replicate/etc (to keep track of which image is which). By default, this will be set to the filename.\n",
    "\n",
    "<b>Important:</b> Note that the images for training in the Ilastik project can't be too large or Ilastik will crash. Around 10000 x 10000 pixels seems to work fine, but smaller (for training at least) is preferred. \n",
    "\n",
    "**2. Feature Selection** </br>\n",
    "- Click `Select Features...`\n",
    "- Select all features\n",
    "- After clicking `OK`, wait until all features have been computed before moving on to the next step.\n",
    "\n",
    "**3. Training** </br>\n",
    "Here, you can create multiple classes for your classifier and train them until you are satisfied with the results. In general, it is useful to initially add a good amount of labels for the different classes for different regions of the image (or even already over multiple images) to capture the variation that is in the data and click `Live Update` to see the prediction results. Subsequently, you can focus more on the mistakes that are being made and add labels to correct those. When labeling, we suggest to unclick `Live Update` to avoid waiting time. It can be useful to use the `Uncertainty` layer to see which pixels are still not robustly trained for. When training, ilastik does not make it easy to change the visualization, but if you right-click on `Raw Input` and select `Adjust Thresholds`, you have some options to change the display.\n",
    "\n",
    "Note that you can save your training labels by right-clicking `Labels` and subsequently `Export...`. Set the `File` to where you want to save the labels and under what name. This entire process needs to be done for every image separately, but it can be worth it to be able to recreate the ilastik project if something goes wrong or if we would want to preprocess the images for example.\n",
    "\n",
    "**4. Prediction Export** </br>\n",
    "- Under `Source`, we have a couple of options to export that can be useful, but we will select `Simple Segmentation` for now. \n",
    "- Click `Choose Export Image Settings...` and set Format to `tiff`. Change the File name to something informative and set `Convert to Data Type` to `integer 8-bit`.\n",
    "- Click `OK` and click `Export All` to export the segmentation mask\n",
    "\n",
    "**5. Batch processing** </br>\n",
    "We will not use the batch processing tab since it is not possible to specify the internal paths in the h5 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Add results to SpatialData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in segmentation mask and clean up mask\n",
    "from skimage import io, morphology\n",
    "\n",
    "mask = io.imread(os.path.join(output_path, 'ROI_segmentation.tiff'))\n",
    "mask = (mask == 1) # Keep only class 1 pixels\n",
    "mask = morphology.remove_small_objects(mask, min_size=15000)\n",
    "mask = morphology.remove_small_holes(mask, area_threshold=15000)\n",
    "mask = mask.astype(np.uint8) # Convert boolean mask back to integers\n",
    "\n",
    "sdata_crop = hp.im.add_labels_layer(\n",
    "    sdata_crop, \n",
    "    arr=mask, \n",
    "    output_layer=\"ilastik_mask\", \n",
    "    chunks=1024, \n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mask\n",
    "sdata_crop.pl.render_labels(\n",
    "    \"ilastik_mask\", \n",
    ").pl.show(\n",
    "    coordinate_systems=\"global\",\n",
    "    figsize=(10, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Superpixel clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create new labels and shapes layers for a hexagonal grid \n",
    "import xarray\n",
    "xa: xarray.DataArray = sdata.images['REAscreen_IO_CRC_image'][f'scale0']['image']\n",
    "shape = (xa.shape[1], xa.shape[2])\n",
    "\n",
    "size = 50 # radius of the hexagon, or size length of the square.\n",
    "\n",
    "sdata = hp.im.add_grid_labels_layer(\n",
    "    sdata, \n",
    "    shape=shape, \n",
    "    size=size, \n",
    "    output_shapes_layer=f\"shapes_spots_{size}um\", \n",
    "    output_labels_layer=f\"labels_spots_{size}um\", \n",
    "    grid_type='hexagon', # Set to 'square' for square grid\n",
    "    offset=(0, 0), \n",
    "    chunks=1024, \n",
    "    client=None, \n",
    "    transformations=None, \n",
    "    scale_factors=(2, 2, 2, 2),\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the grid\n",
    "hp.pl.plot_shapes( \n",
    "    sdata,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    shapes_layer=f\"shapes_spots_{size}um\",\n",
    "    channel=\"DAPI (1)\",\n",
    "    linewidth=0.4,\n",
    "    alpha=0.3,\n",
    "    figsize=(14,10),\n",
    "    to_coordinate_system=\"global\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate intensities \n",
    "sdata = hp.tb.allocate_intensity( \n",
    "    sdata,\n",
    "    img_layer= \"REAscreen_IO_CRC_image\",\n",
    "    labels_layer=f\"labels_spots_{size}um\",\n",
    "    output_layer=\"table_superpixel_intensities\",\n",
    "    channels=marker_channels,\n",
    "    mode=\"sum\", # Can be set to \"mean\" or \"sum\"\n",
    "    to_coordinate_system=\"global\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess \n",
    "sdata=hp.tb.preprocess_proteomics(\n",
    "    sdata,\n",
    "    labels_layer=f\"labels_spots_{size}um\",\n",
    "    table_layer=\"table_superpixel_intensities\",\n",
    "    output_layer=\"table_superpixel_intensities_prepocessed\",\n",
    "    size_norm=True,\n",
    "    log1p=True,\n",
    "    scale=False,\n",
    "    max_value_scale=10,\n",
    "    q=None,\n",
    "    calculate_pca=False,\n",
    "    n_comps=50,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Leiden clustering\n",
    "sdata = hp.tb.leiden(\n",
    "    sdata,\n",
    "    labels_layer=f\"labels_spots_{size}um\",\n",
    "    table_layer=\"table_superpixel_intensities_prepocessed\",\n",
    "    output_layer=\"table_superpixel_intensities_leiden\",\n",
    "    calculate_umap=True,\n",
    "    calculate_neighbors=True,\n",
    "    n_pcs=17, # The number of principal components to use when calculating neighbors.\n",
    "    n_neighbors=35, # The number of neighbors to consider when calculating neighbors.\n",
    "    resolution=0.4,\n",
    "    rank_genes=True,\n",
    "    key_added=\"leiden\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Plot UMAP\n",
    "sc.pl.umap(sdata.tables[\"table_superpixel_intensities_leiden\"], color=[\"leiden\"], show=True)\n",
    "\n",
    "# Plot rank plot\n",
    "sc.pl.rank_genes_groups(\n",
    "    sdata.tables[\"table_superpixel_intensities_leiden\"],\n",
    "    n_genes=8,\n",
    "    sharey=False,\n",
    "    show=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Leiden clusters spatially\n",
    "hp.pl.plot_shapes( \n",
    "    sdata,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    table_layer=\"table_superpixel_intensities_leiden\",\n",
    "    shapes_layer=f\"shapes_spots_{size}um\",\n",
    "    column=\"leiden\",\n",
    "    cmap=\"rainbow\",\n",
    "    channel=\"DAPI (1)\",\n",
    "    linewidth=0.2,\n",
    "    alpha=0.7,\n",
    "    figsize=(14,10),\n",
    "    to_coordinate_system=\"global\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot expression of CD11c\n",
    "hp.pl.plot_shapes( \n",
    "    sdata,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    table_layer=\"table_superpixel_intensities_leiden\",\n",
    "    shapes_layer=f\"shapes_spots_{size}um\",\n",
    "    column=\"CD11c\",\n",
    "    channel=\"DAPI (1)\",\n",
    "    linewidth=0.2,\n",
    "    alpha=0.7,\n",
    "    figsize=(14,10),\n",
    "    to_coordinate_system=\"global\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix plot\n",
    "sc.pl.matrixplot(\n",
    "    sdata.tables[\"table_superpixel_intensities_leiden\"], \n",
    "    var_names=marker_channels, \n",
    "    groupby=\"leiden\", \n",
    "    cmap=\"Blues\",\n",
    "    standard_scale=\"var\",\n",
    "    colorbar_title=\"column scaled\\nexpression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pixel clustering (FlowSOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a crop to speed up the computation time for the workshop\n",
    "from spatialdata import bounding_box_query\n",
    "\n",
    "se = bounding_box_query(\n",
    "    sdata[\"REAscreen_IO_CRC_image\"],\n",
    "    axes=(\"y\", \"x\"),\n",
    "    min_coordinate=[0, 5000], # y_min, x_min\n",
    "    max_coordinate=[5000, 10000], # y_max, x_max\n",
    "    target_coordinate_system=\"global\",\n",
    ")\n",
    "\n",
    "sdata.images[\"REAscreen_IO_CRC_image_flowsom_crop\"] = se\n",
    "sdata.write_element(\n",
    "    \"REAscreen_IO_CRC_image_flowsom_crop\", overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROI layer on crop\n",
    "hp.pl.plot_shapes(\n",
    "    sdata_crop, \n",
    "    img_layer='REAscreen_IO_CRC_image', \n",
    "    channel='DAPI (1)', \n",
    "    shapes_layer='ROI',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preprocessing step normalizes and blurs the images based on various quantile and gaussian blur parameters\n",
    "sdata_crop = hp.im.pixel_clustering_preprocess(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    output_layer=\"REAscreen_IO_CRC_image_flowsom_preprocessed\",\n",
    "    channels=successful_channels,\n",
    "    q=99, # Quantile used for normalization. Each channel is normalized by its own calculated quantile.\n",
    "    q_sum=5, # If the sum of the channel values at a pixel is below this quantile, the pixel values across all channels are set to NaN.\n",
    "    q_post=99.9, # Quantile used for normalization after other preprocessing steps (`q`, `q_sum`, `norm_sum` normalization and Gaussian blurring) are performed.\n",
    "    sigma=2, # Gaussian blur parameter for each channel. Use `0` to omit blurring for specific channels or `None` to skip blurring altogether.\n",
    "    norm_sum=True, # If `True`, each channel is normalized by the sum of all channels at each pixel.\n",
    "    cap_max=1, # The maximum allowable value for the elements in the resulting preprocessed image layers. If `None`, no capping is applied. Typical value would be `1.0` to exclude outliers.\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the histogram for CD4 after preprocessing with the original image\n",
    "channel = \"CD4\"\n",
    "\n",
    "hp.pl.histogram(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image\",\n",
    "    channel=channel,\n",
    "    bins=100,\n",
    "    fig_kwargs={\n",
    "        \"figsize\": (4, 4),\n",
    "    },\n",
    ")\n",
    "\n",
    "hp.pl.histogram(\n",
    "    sdata_crop,\n",
    "    img_layer=\"REAscreen_IO_CRC_image_flowsom_preprocessed\",\n",
    "    channel=channel,\n",
    "    bins=100,\n",
    "    fig_kwargs={\n",
    "        \"figsize\": (4, 4),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 FlowSOM pixel clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlowSOM clustering\n",
    "\n",
    "import flowsom as fs\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "work_with_client = False\n",
    "\n",
    "if work_with_client:\n",
    "    # client example\n",
    "    cluster = LocalCluster(\n",
    "        n_workers=1,\n",
    "        threads_per_worker=10,\n",
    "    )\n",
    "\n",
    "    client = Client(cluster)\n",
    "else:\n",
    "    client = None\n",
    "\n",
    "batch_model = fs.models.BatchFlowSOMEstimator\n",
    "\n",
    "sdata_crop, fsom, mapping = hp.im.flowsom(\n",
    "    sdata_crop,\n",
    "    img_layer=[\"REAscreen_IO_CRC_image_flowsom_preprocessed\"],\n",
    "    output_layer_clusters=[\n",
    "        \"flowsom_clusters\",\n",
    "    ],  # we need output_cluster_layer and output_meta_cluster_layer --> these will both be labels layers\n",
    "    output_layer_metaclusters=[\n",
    "        \"flowsom_metaclusters\",\n",
    "    ],\n",
    "    channels = successful_channels, \n",
    "    fraction = 0.1, # Fraction of the data to sample for training flowsom.\n",
    "    n_clusters=20, # The number of meta clusters to form.\n",
    "    random_state=111,\n",
    "    chunks=512,\n",
    "    client=client,\n",
    "    model=batch_model,\n",
    "    num_batches=10,\n",
    "    xdim=10,\n",
    "    ydim=10,\n",
    "    z_score=False,\n",
    "    z_cap=3,\n",
    "    persist_intermediate=True,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all FlowSOM clusters\n",
    "hp.pl.pixel_clusters(\n",
    "    sdata_crop,\n",
    "    labels_layer=\"flowsom_clusters\",\n",
    "    figsize=(14, 10),\n",
    "    to_coordinate_system=\"global\",\n",
    "    render_labels_kwargs={\"alpha\": 1, \"cmap\": \"rainbow\", \"to_coordinate_system\": \"global\"},\n",
    "    coordinate_systems=\"global\", # passed to .pl.show()\n",
    ")\n",
    "\n",
    "# Plot FlowSOM metaclusters\n",
    "hp.pl.pixel_clusters(\n",
    "    sdata_crop,\n",
    "    labels_layer=\"flowsom_metaclusters\",\n",
    "    figsize=(14, 10),\n",
    "    to_coordinate_system=\"global\",\n",
    "    render_labels_kwargs={\"alpha\": 1, \"cmap\": \"rainbow\", \"to_coordinate_system\": \"global\"},\n",
    "    coordinate_systems=\"global\", # passed to .pl.show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average intensities per SOM cluster\n",
    "sdata_crop = hp.tb.cluster_intensity( # This function computes average intensity for each SOM cluster identified in the `labels_layer` and stores the results in a new table layer (`output_layer`).\n",
    "    sdata_crop,\n",
    "    mapping=mapping,\n",
    "    img_layer=[\"REAscreen_IO_CRC_image_flowsom_preprocessed\"],\n",
    "    labels_layer=[\"flowsom_clusters\"],\n",
    "    to_coordinate_system=[\"global\"],\n",
    "    output_layer=\"counts_clusters\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmaps for FlowSOM clusters and metaclusters\n",
    "for _metaclusters in [True]:\n",
    "    hp.pl.pixel_clusters_heatmap(\n",
    "        sdata_crop,\n",
    "        table_layer=\"counts_clusters\",\n",
    "        figsize=(40, 16),\n",
    "        fig_kwargs={\"dpi\": 300},\n",
    "        linewidths=0.001,\n",
    "        metaclusters=_metaclusters,\n",
    "        z_score=True,\n",
    "        output='img/FlowSOM_full_image_heatmap.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran FlowSOM pixel clustering on the entire image using the same settings and n_clusters set to 15. You can find the results below:\n",
    "\n",
    "![Alt text](img/FlowSOM_full_image_spatial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Spatial enrichment FlowSOM metaclusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use hp.tb.spatial_pixel_neighbors() after calculating FlowSOM metaclusters to get neighborhood enrichment scores among metaclusters. This function extracts grid-based cluster labels from the specified labels layer of a SpatialData object,\n",
    "subdivides the spatial domain into a grid using a specified sampling interval, and computes spatial neighbors along with\n",
    "neighborhood enrichment statistics. The resulting AnnData object stores the cluster labels as a categorical\n",
    "observation (under the key provided by `key_added`) and the corresponding spatial coordinates in its `.obsm`\n",
    "attribute. `squidpy` is used for the spatial neighbors computation and\n",
    "the neighborhood enrichment analysis (i.e. `squidpy.gr.spatial_neighbors` and `squidpy.gr.nhood_enrichment`).\n",
    "Results can then be visualized using e.g. `squidpy.pl.nhood_enrichment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import squidpy as sq\n",
    "\n",
    "key_added = \"cluster_id\"\n",
    "\n",
    "adata = hp.tb.spatial_pixel_neighbors(\n",
    "    sdata_crop,\n",
    "    labels_layer=\"flowsom_metaclusters\",\n",
    "    key_added=key_added,\n",
    "    mode=\"most_frequent\",\n",
    "    grid_type=\"hexagon\",\n",
    "    size=20,\n",
    "    subset=None,\n",
    ")\n",
    "\n",
    "adata.uns[f\"{key_added}_nhood_enrichment\"][\"zscore\"] = np.nan_to_num(\n",
    "    adata.uns[f\"{key_added}_nhood_enrichment\"][\"zscore\"]\n",
    ")\n",
    "sq.pl.nhood_enrichment(adata, cluster_key=key_added, method=\"ward\", mode=\"zscore\", figsize=(8, 8))\n",
    "\n",
    "# NOTE: cluster_id 0 contains all pixels that were filtered out during preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cell clustering (FlowSOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign cell types based on FlowSOM pixel metaclusters\n",
    "\n",
    "batch_model = fs.models.BatchFlowSOMEstimator\n",
    "\n",
    "sdata_crop, fsom = hp.tb.flowsom(\n",
    "    sdata_crop,\n",
    "    labels_layer_cells=[\"labels_cells_instanseg_artifacts_filtered\"],\n",
    "    labels_layer_clusters=[\"flowsom_metaclusters\"],\n",
    "    output_layer=\"table_cell_clustering_flowsom\",\n",
    "    n_clusters=15,\n",
    "    chunks=512,\n",
    "    model=batch_model,\n",
    "    num_batches=10,\n",
    "    random_state=100,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect table layer\n",
    "sdata_crop.tables['table_cell_clustering_flowsom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect obs\n",
    "sdata_crop.tables['table_cell_clustering_flowsom'].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cells colored by FlowSOM cluster\n",
    "sdata_crop.pl.render_labels(\n",
    "    \"labels_cells_instanseg_artifacts_filtered\", \n",
    "    table_name=\"table_cell_clustering_flowsom\", \n",
    "    color=\"clustering\", \n",
    "    cmap=\"rainbow\"\n",
    ").pl.show(\n",
    "    coordinate_systems=\"global\",\n",
    "    figsize=(10, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cells colored by FlowSOM metacluster\n",
    "sdata_crop.pl.render_labels(\n",
    "    \"labels_cells_instanseg_artifacts_filtered\", \n",
    "    table_name=\"table_cell_clustering_flowsom\", \n",
    "    color=\"metaclustering\", \n",
    "    cmap=\"rainbow\"\n",
    ").pl.show(\n",
    "    coordinate_systems=\"global\",\n",
    "    figsize=(10, 10)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
